{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import and install dependencies"
      ],
      "metadata": {
        "id": "Av-udXuAOvG2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ra3T7iLim-A",
        "outputId": "81f7fb5a-707e-4fdc-cf26-854d320f2355",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Collecting scikit-network\n",
            "  Downloading scikit_network-0.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scikit-network) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.11/dist-packages (from scikit-network) (1.13.1)\n",
            "Downloading scikit_network-0.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-network\n",
            "Successfully installed scikit-network-0.33.1\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install scikit-network\n",
        "!pip install python-dotenv\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "import os\n",
        "import spacy\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import SVG\n",
        "from sknetwork.data import Bunch\n",
        "from sknetwork.ranking import PageRank\n",
        "from sknetwork.visualization import svg_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using **The Guardian** API to Search for Recent Articles Related to a Subject (e.g., Bitcoin)\n",
        "\n",
        "The /search endpoint allows you to query articles with optional filters such as `tag` and `section` to refine your results.\n",
        "\n",
        "By default, you can use the **\"test\"** API key provided by The Guardian for experimentation and basic queries. However, if you plan to use this API for a project, it is highly recommended to sign up for your own API key to benefit from higher request limits and additional features.  \n",
        "\n",
        "Visit [The Guardian Developer website](https://open-platform.theguardian.com/) and register for a free API key.\n",
        "\n",
        "For more details on how to use this API effectively, refer to the [official documentation](https://open-platform.theguardian.com/documentation/).\n"
      ],
      "metadata": {
        "id": "tbmELgiYRoRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search for tags associated with a given term using **The Guardian** API.  "
      ],
      "metadata": {
        "id": "qzAL6FcVR-cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_tags(term, api_key=\"test\"):\n",
        "    \"\"\"\n",
        "    Search for tags related to a specific term using The Guardian API.\n",
        "\n",
        "    Parameters:\n",
        "        term (str): The term to search for.\n",
        "        api_key (str): The API key for The Guardian API. Defaults to \"test\".\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tag IDs related to the term, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    base_url = \"https://content.guardianapis.com/tags\"\n",
        "    params = {\n",
        "        \"q\": term,\n",
        "        \"api-key\": api_key\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        tags_data = response.json()\n",
        "        if tags_data:\n",
        "            return [tag[\"id\"] for tag in tags_data[\"response\"][\"results\"]]\n",
        "\n",
        "        return []\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching tags: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "HE1UdkEjRNAU"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch the Latest Article URLs from **The Guardian** API based on a search term (`query`)\n",
        "\n"
      ],
      "metadata": {
        "id": "cgRB5KuASgXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_latest_guardian_articles_urls(query, api_key=\"test\", page_size=10, tag=None, section=None):\n",
        "    \"\"\"\n",
        "    Fetch the latest article URLs related to a specific query from The Guardian API.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The term to search for.\n",
        "        api_key (str): The API key for The Guardian API. Defaults to \"test\".\n",
        "        page_size (int): Number of articles to return. Defaults to 10.\n",
        "        tag (str, optional): A tag to filter results (e.g., \"technology/bitcoin\").\n",
        "        section (str, optional): A section to filter results (e.g., \"economy\").\n",
        "\n",
        "    Returns:\n",
        "        list: A list of article URLs, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    base_url = \"https://content.guardianapis.com/search\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"order-by\": \"newest\",\n",
        "        \"page-size\": page_size,\n",
        "        \"api-key\": api_key\n",
        "    }\n",
        "\n",
        "    if tag:\n",
        "        params[\"tag\"] = tag\n",
        "        #print(f\"Using tag: {tag}\")\n",
        "\n",
        "    if section:\n",
        "        params[\"section\"] = section\n",
        "        #print(f\"Using section: {section}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        #print(f\"Request URL: {response.url}\")\n",
        "\n",
        "        articles_data = response.json()\n",
        "        if articles_data and articles_data['response']['results']:\n",
        "            return [article[\"webUrl\"] for article in articles_data['response']['results']]\n",
        "\n",
        "        print(\"No articles found.\")\n",
        "        return []\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching articles: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "ATYywYT-St0j"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using spaCy and Natural Language Toolkit (NLTK) to Process Articles from HTML Pages\n",
        "\n",
        "This section demonstrates how to use **spaCy** and **NLTK** to process and analyze the content of articles retrieved using **The Guardian API**.  \n",
        "\n",
        "#### Overview:\n",
        "\n",
        "1. Text Extraction: parse the HTML obtained from The Guardian API URLs to extract the main article text.\n",
        "2. Natural Language Processing (NLP):\n",
        "   - **spaCy** used for tokenization, named entity recognition (NER), and part-of-speech (POS) tagging.\n",
        "   - **NLTK** used for part-of-speech (POS) tagging."
      ],
      "metadata": {
        "id": "uryZBVhtbln4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HTML parse and cleaning text article from given a given URL"
      ],
      "metadata": {
        "id": "E45h2mp5VjP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_article_text(url):\n",
        "    \"\"\"\n",
        "    Fetches and extracts the article text from a given URL.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the article to extract text from.\n",
        "\n",
        "    Returns:\n",
        "        str: The text content of the article, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raises an HTTPError for bad responses (4xx, 5xx)\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Only match <article> tag to get the main content\n",
        "        article = soup.find('article')\n",
        "        if not article:\n",
        "            print(f\"No <article> tag found for URL: {url}\")\n",
        "            return None\n",
        "\n",
        "        # Find all relevant text elements (<p> for paragraphs, <h1>-<h6> for headings)\n",
        "        tags = article.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "\n",
        "        # Extract text and join it into a single string\n",
        "        text = ''.join(tag.get_text(strip=True).rstrip('.\\n') + \".\\n\" for tag in tags)\n",
        "\n",
        "        return text\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error processing URL {url}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ePqVsg8kdtkp"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using only spaCy for tokenization, named entity recognition (NER), and part-of-speech (POS) tagging."
      ],
      "metadata": {
        "id": "iBwJu_qCemQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_with_spacy(text):\n",
        "    \"\"\"\n",
        "    Processes the given text using spaCy for tokenization, part-of-speech tagging,\n",
        "    and named entity recognition (NER).\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to process.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two elements:\n",
        "            - A list of proper noun tags (tokens with tag 'NNP').\n",
        "            - A list of filtered named entities (PERSON, ORG, GPE).\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "\n",
        "    sentences = list(doc.sents)\n",
        "\n",
        "    entities = []\n",
        "    nnp_tags = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "      for token in sentence:\n",
        "        if token.tag_ == 'NNP' and token.text.isalpha():\n",
        "          nnp_tags.append(token.text)\n",
        "\n",
        "      sentence_entities = []\n",
        "\n",
        "      sent_doc = nlp(sentence.text)\n",
        "      for ent in sent_doc.ents:\n",
        "        if ent.label_ in ['PERSON', 'ORG', 'GPE']:\n",
        "          entity = ent.text.strip()\n",
        "\n",
        "          if \"'s\" in entity:\n",
        "\n",
        "              cutoff = entity.index(\"'s\")\n",
        "\n",
        "              entity = entity[:cutoff]\n",
        "\n",
        "          if entity != '':\n",
        "\n",
        "              sentence_entities.append(entity)\n",
        "\n",
        "        sentence_entities = list(set(sentence_entities))\n",
        "\n",
        "        if len(sentence_entities) > 1:\n",
        "\n",
        "            entities.append(sentence_entities)\n",
        "\n",
        "    return nnp_tags, entities"
      ],
      "metadata": {
        "id": "9xTQ8nDXyHdg"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using spaCy and NLTK for tokenization, named entity recognition (NER), and part-of-speech (POS) tagging."
      ],
      "metadata": {
        "id": "U6Nf9jMVev0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_with_nltk_and_spacy(text):\n",
        "    \"\"\"\n",
        "    Process the given text using both NLTK and spaCy for tokenization,\n",
        "    part-of-speech tagging, and named entity recognition (NER).\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to be processed.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - A list of proper nouns (NNP) from POS tagging using NLTK.\n",
        "            - A list of filtered named entities (PERSON, ORG, GPE) using spaCy.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        #NLTK\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "        pos_tags = []\n",
        "        for sentence in sentences:\n",
        "            words = nltk.word_tokenize(sentence)\n",
        "            pos_tags.extend(nltk.pos_tag(words))\n",
        "\n",
        "        nnp_tags = [tag for tag in pos_tags if tag[1] == 'NNP' and tag[0].isalpha()]\n",
        "\n",
        "        #spaCy\n",
        "        doc = nlp(text)\n",
        "\n",
        "        sentences = list(doc.sents)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "          sentence_entities = []\n",
        "\n",
        "          sent_doc = nlp(sentence.text)\n",
        "          for ent in sent_doc.ents:\n",
        "            if ent.label_ in ['PERSON', 'ORG', 'GPE']:\n",
        "              entity = ent.text.strip()\n",
        "\n",
        "              if \"'s\" in entity:\n",
        "\n",
        "                  cutoff = entity.index(\"'s\")\n",
        "\n",
        "                  entity = entity[:cutoff]\n",
        "\n",
        "              if entity != '':\n",
        "\n",
        "                  sentence_entities.append(entity)\n",
        "\n",
        "            sentence_entities = list(set(sentence_entities))\n",
        "\n",
        "            if len(sentence_entities) > 1:\n",
        "\n",
        "                entities.append(sentence_entities)\n",
        "\n",
        "        return nnp_tags, entities\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {e}\")\n",
        "        return [], []\n"
      ],
      "metadata": {
        "id": "VX4CBKo970l-"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting entities into network data\n"
      ],
      "metadata": {
        "id": "HOBPog1wf-Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract entities from articles_urls texts"
      ],
      "metadata": {
        "id": "E7l1YWypKa38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(articles_urls, print_all=False):\n",
        "    \"\"\"\n",
        "    Extracts named entities related to theme from a list of article URLs.\n",
        "\n",
        "    Parameters:\n",
        "        articles_urls (list of str): List of URLs to process.\n",
        "        print_all (bool): If True, prints URLs, entities and NNP Tags.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of named entities extracted from the articles.\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "\n",
        "    for url in articles_url:\n",
        "        text = get_article_text(url)\n",
        "\n",
        "        if print_all:\n",
        "            print(f\"URL: {url}\")\n",
        "\n",
        "        if text:\n",
        "            #nnp_tags, article_entities = process_text_with_nltk_and_spacy(text)\n",
        "            nnp_tags, article_entities = process_text_with_spacy(text)\n",
        "            entities.extend(article_entities)\n",
        "\n",
        "            if print_all:\n",
        "                print(\"NNP Tags (Proper Nouns):\", nnp_tags)\n",
        "                print(f\"Filtered Named Entities (PERSON, ORG, GPE): {article_entities}\\n\")\n",
        "\n",
        "    return entities"
      ],
      "metadata": {
        "id": "5YR2QPamKZSo"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get network data from entities"
      ],
      "metadata": {
        "id": "rql3opC2MQPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network_data(entities):\n",
        "\n",
        "    final_sources = []\n",
        "    final_targets = []\n",
        "\n",
        "    for row in entities:\n",
        "\n",
        "        source = row[0]\n",
        "        targets = row[1:]\n",
        "\n",
        "        for target in targets:\n",
        "\n",
        "            final_sources.append(source)\n",
        "            final_targets.append(target)\n",
        "\n",
        "    df = pd.DataFrame({'source':final_sources, 'target':final_targets})\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "R7GonV0hL-zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draw networkx graphs from network data"
      ],
      "metadata": {
        "id": "3gld6PBaPQOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_graph(G, show_names=False, node_size=1, font_size=10, edge_width=0.5):\n",
        "\n",
        "    adjacency = nx.adjacency_matrix(G, weight='weight')\n",
        "    adjacency = sp.csr_matrix(adjacency)\n",
        "\n",
        "    names = np.array(list(G.nodes()))\n",
        "\n",
        "    graph = Bunch()\n",
        "    graph.adjacency = adjacency\n",
        "    graph.names = np.array(names)\n",
        "\n",
        "    pagerank = PageRank()\n",
        "\n",
        "    pagerank.fit(adjacency)\n",
        "    scores = pagerank.scores_\n",
        "\n",
        "    if show_names:\n",
        "\n",
        "        image = svg_graph(graph.adjacency, font_size=font_size, node_size=node_size, names=graph.names, width=700, height=500, scores=scores, edge_width=edge_width)\n",
        "\n",
        "    else:\n",
        "\n",
        "        image = svg_graph(graph.adjacency, node_size=node_size, width=700, height=500, scores = scores, edge_width=edge_width)\n",
        "\n",
        "    return SVG(image)"
      ],
      "metadata": {
        "id": "Imr9fk0zPYDb"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_ego_graph(G, ego, center=True, k=0, show_names=True, edge_width=0.1, node_size=3, font_size=12):\n",
        "\n",
        "    G.remove_edges_from(nx.selfloop_edges(G))\n",
        "\n",
        "    ego = nx.ego_graph(G, ego, center=center)\n",
        "\n",
        "    ego = nx.k_core(ego, k)\n",
        "\n",
        "    return draw_graph(ego, node_size=node_size, font_size=font_size, show_names=show_names, edge_width=edge_width)"
      ],
      "metadata": {
        "id": "TSI9PLESRFZQ"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure environment, set query and apply functions to get graph and metrics"
      ],
      "metadata": {
        "id": "ctk7SlvYNhjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "api_key = os.getenv(\"API_KEY\", \"test\")\n",
        "\n",
        "query = \"bitcoin\"\n",
        "\n",
        "tag = search_tags(query, api_key)[0]  # Get first tag\n",
        "section = \"money\"  # You can also use technology, economy or omit it\n",
        "\n",
        "articles_url = get_latest_guardian_articles_urls(query, api_key, page_size=100, tag=tag, section=None)\n",
        "\n",
        "print(len(articles_url))\n",
        "\n",
        "bitcoin_entities = extract_entities(articles_url, print_all=False)\n",
        "\n",
        "bitcoin_network_df = get_network_data(bitcoin_entities)\n",
        "\n",
        "G_bitcoin = nx.from_pandas_edgelist(bitcoin_network_df)\n"
      ],
      "metadata": {
        "id": "m5yGS9eJO4xJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43a49f59-df79-4731-aa06-bf5bc19bd643"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "draw_ego_graph(G_bitcoin, 'Bitcoin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "VLEjIr0eRd6L",
        "outputId": "18e1450d-f090-4659-95ee-b30e069d02a0"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"981.5185373538677\" height=\"540\">\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 720 293 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 330 520 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 203 65 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 203 65 121 114\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 342 20 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 517 26 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 614 370 720 293\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 614 370 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 614 370 529 425\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 614 370 459 391\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 614 370 526 282\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 140 246 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 459 391 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 459 391 529 425\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 39 332 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 459 391 614 370\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 88 421 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 204 458 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 651 231 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 651 231 526 282\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 529 142 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 529 142 526 282\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 526 282 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 526 282 529 425\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 526 282 396 163\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 526 282 614 370\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 526 282 459 391\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 526 282 651 231\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 526 282 529 142\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 459 391 526 282\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 396 163 526 282\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 396 163 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 593 506 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 720 293 698 164\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 720 293 614 370\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 720 293\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 529 425\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 698 164\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 20 215\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 593 506\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 396 163\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 39 332\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 330 520\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 203 65\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 342 20\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 517 26\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 614 370\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 140 246\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 459 391\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 88 421\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 204 458\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 651 231\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 529 142\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 526 282\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 370 275 121 114\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 529 425 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 529 425 614 370\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 529 425 459 391\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 529 425 526 282\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 698 164 720 293\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 698 164 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 20 215 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 121 114 370 275\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 121 114 203 65\"/>\n<circle cx=\"720\" cy=\"293\" r=\"3.0\" style=\"fill:rgb(86, 115, 224);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"370\" cy=\"275\" r=\"3.0\" style=\"fill:rgb(179, 3, 38);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"529\" cy=\"425\" r=\"3.0\" style=\"fill:rgb(96, 128, 232);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"698\" cy=\"164\" r=\"3.0\" style=\"fill:rgb(70, 93, 207);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"20\" cy=\"215\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"593\" cy=\"506\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"396\" cy=\"163\" r=\"3.0\" style=\"fill:rgb(70, 93, 207);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"39\" cy=\"332\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"330\" cy=\"520\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"203\" cy=\"65\" r=\"3.0\" style=\"fill:rgb(72, 96, 209);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"342\" cy=\"20\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"517\" cy=\"26\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"614\" cy=\"370\" r=\"3.0\" style=\"fill:rgb(109, 144, 241);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"140\" cy=\"246\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"459\" cy=\"391\" r=\"3.0\" style=\"fill:rgb(96, 128, 232);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"88\" cy=\"421\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"204\" cy=\"458\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"651\" cy=\"231\" r=\"3.0\" style=\"fill:rgb(70, 93, 207);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"529\" cy=\"142\" r=\"3.0\" style=\"fill:rgb(70, 93, 207);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"526\" cy=\"282\" r=\"3.0\" style=\"fill:rgb(145, 179, 254);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"121\" cy=\"114\" r=\"3.0\" style=\"fill:rgb(72, 96, 209);stroke:black;stroke-width:1.0\"/>\n<text text-anchor=\"start\" x=\"726\" y=\"293\" font-size=\"12\">FBI</text><text text-anchor=\"start\" x=\"376\" y=\"275\" font-size=\"12\">Bitcoin</text><text text-anchor=\"start\" x=\"535\" y=\"425\" font-size=\"12\">America</text><text text-anchor=\"start\" x=\"704\" y=\"164\" font-size=\"12\">Gurvais Grigg</text><text text-anchor=\"start\" x=\"26\" y=\"215\" font-size=\"12\">Joseph James O’Connor</text><text text-anchor=\"start\" x=\"599\" y=\"506\" font-size=\"12\">Morgan</text><text text-anchor=\"start\" x=\"402\" y=\"163\" font-size=\"12\">UK</text><text text-anchor=\"start\" x=\"45\" y=\"332\" font-size=\"12\">Binance boss</text><text text-anchor=\"start\" x=\"336\" y=\"520\" font-size=\"12\">Brosens</text><text text-anchor=\"start\" x=\"209\" y=\"65\" font-size=\"12\">authorof</text><text text-anchor=\"start\" x=\"348\" y=\"20\" font-size=\"12\">David Gerard</text><text text-anchor=\"start\" x=\"523\" y=\"26\" font-size=\"12\">US Securities and Exchange Commission</text><text text-anchor=\"start\" x=\"620\" y=\"370\" font-size=\"12\">SEC</text><text text-anchor=\"start\" x=\"146\" y=\"246\" font-size=\"12\">Monero</text><text text-anchor=\"start\" x=\"465\" y=\"391\" font-size=\"12\">Trump</text><text text-anchor=\"start\" x=\"94\" y=\"421\" font-size=\"12\">Sam Bankman-Friedcommitted</text><text text-anchor=\"start\" x=\"210\" y=\"458\" font-size=\"12\">reserve”of</text><text text-anchor=\"start\" x=\"657\" y=\"231\" font-size=\"12\">Celsius</text><text text-anchor=\"start\" x=\"535\" y=\"142\" font-size=\"12\">Donald Trump</text><text text-anchor=\"start\" x=\"532\" y=\"282\" font-size=\"12\">US</text><text text-anchor=\"start\" x=\"127\" y=\"114\" font-size=\"12\">Satoshi Nakamoto</text></svg>"
          },
          "metadata": {},
          "execution_count": 268
        }
      ]
    }
  ]
}