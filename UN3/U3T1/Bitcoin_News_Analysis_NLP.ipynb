{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Import and install dependencies"
      ],
      "metadata": {
        "id": "Av-udXuAOvG2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ra3T7iLim-A",
        "outputId": "81f7fb5a-707e-4fdc-cf26-854d320f2355",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.11/dist-packages (3.0.1)\n",
            "Collecting scikit-network\n",
            "  Downloading scikit_network-0.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from scikit-network) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.11/dist-packages (from scikit-network) (1.13.1)\n",
            "Downloading scikit_network-0.33.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-network\n",
            "Successfully installed scikit-network-0.33.1\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install scikit-network\n",
        "!pip install python-dotenv\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "import os\n",
        "import spacy\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "from bs4 import BeautifulSoup\n",
        "from dotenv import load_dotenv\n",
        "from IPython.display import SVG\n",
        "from sknetwork.data import Bunch\n",
        "from sknetwork.ranking import PageRank\n",
        "from sknetwork.visualization import svg_graph"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using **The Guardian** API to Search for Recent Articles Related to a Subject (e.g., Bitcoin)\n",
        "\n",
        "The /search endpoint allows you to query articles with optional filters such as `tag` and `section` to refine your results.\n",
        "\n",
        "By default, you can use the **\"test\"** API key provided by The Guardian for experimentation and basic queries. However, if you plan to use this API for a project, it is highly recommended to sign up for your own API key to benefit from higher request limits and additional features.  \n",
        "\n",
        "Visit [The Guardian Developer website](https://open-platform.theguardian.com/) and register for a free API key.\n",
        "\n",
        "For more details on how to use this API effectively, refer to the [official documentation](https://open-platform.theguardian.com/documentation/).\n"
      ],
      "metadata": {
        "id": "tbmELgiYRoRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search for tags associated with a given term using **The Guardian** API.  "
      ],
      "metadata": {
        "id": "qzAL6FcVR-cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_tags(term, api_key=\"test\"):\n",
        "    \"\"\"\n",
        "    Search for tags related to a specific term using The Guardian API.\n",
        "\n",
        "    Parameters:\n",
        "        term (str): The term to search for.\n",
        "        api_key (str): The API key for The Guardian API. Defaults to \"test\".\n",
        "\n",
        "    Returns:\n",
        "        list: A list of tag IDs related to the term, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    base_url = \"https://content.guardianapis.com/tags\"\n",
        "    params = {\n",
        "        \"q\": term,\n",
        "        \"api-key\": api_key\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        tags_data = response.json()\n",
        "        if tags_data:\n",
        "            return [tag[\"id\"] for tag in tags_data[\"response\"][\"results\"]]\n",
        "\n",
        "        return []\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching tags: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "HE1UdkEjRNAU"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch the Latest Article URLs from **The Guardian** API based on a search term (`query`)\n",
        "\n"
      ],
      "metadata": {
        "id": "cgRB5KuASgXh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_latest_guardian_articles_urls(query, api_key=\"test\", page_size=10, tag=None, section=None):\n",
        "    \"\"\"\n",
        "    Fetch the latest article URLs related to a specific query from The Guardian API.\n",
        "\n",
        "    Parameters:\n",
        "        query (str): The term to search for.\n",
        "        api_key (str): The API key for The Guardian API. Defaults to \"test\".\n",
        "        page_size (int): Number of articles to return. Defaults to 10.\n",
        "        tag (str, optional): A tag to filter results (e.g., \"technology/bitcoin\").\n",
        "        section (str, optional): A section to filter results (e.g., \"economy\").\n",
        "\n",
        "    Returns:\n",
        "        list: A list of article URLs, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    base_url = \"https://content.guardianapis.com/search\"\n",
        "    params = {\n",
        "        \"q\": query,\n",
        "        \"order-by\": \"newest\",\n",
        "        \"page-size\": page_size,\n",
        "        \"api-key\": api_key\n",
        "    }\n",
        "\n",
        "    if tag:\n",
        "        params[\"tag\"] = tag\n",
        "        #print(f\"Using tag: {tag}\")\n",
        "\n",
        "    if section:\n",
        "        params[\"section\"] = section\n",
        "        #print(f\"Using section: {section}\")\n",
        "\n",
        "    try:\n",
        "        response = requests.get(base_url, params=params)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        #print(f\"Request URL: {response.url}\")\n",
        "\n",
        "        articles_data = response.json()\n",
        "        if articles_data and articles_data['response']['results']:\n",
        "            return [article[\"webUrl\"] for article in articles_data['response']['results']]\n",
        "\n",
        "        print(\"No articles found.\")\n",
        "        return []\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching articles: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "ATYywYT-St0j"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using spaCy and Natural Language Toolkit (NLTK) to Process Articles from HTML Pages\n",
        "\n",
        "This section demonstrates how to use **spaCy** and **NLTK** to process and analyze the content of articles retrieved using **The Guardian API**.  \n",
        "\n",
        "#### Overview:\n",
        "\n",
        "1. Text Extraction: parse the HTML obtained from The Guardian API URLs to extract the main article text.\n",
        "2. Natural Language Processing (NLP):\n",
        "   - **spaCy** used for tokenization, named entity recognition (NER), and part-of-speech (POS) tagging.\n",
        "   - **NLTK** used for part-of-speech (POS) tagging."
      ],
      "metadata": {
        "id": "uryZBVhtbln4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HTML parse and cleaning text article from given a given URL"
      ],
      "metadata": {
        "id": "E45h2mp5VjP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_article_text(url):\n",
        "    \"\"\"\n",
        "    Fetches and extracts the article text from a given URL.\n",
        "\n",
        "    Parameters:\n",
        "        url (str): The URL of the article to extract text from.\n",
        "\n",
        "    Returns:\n",
        "        str: The text content of the article, or None if an error occurs.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raises an HTTPError for bad responses (4xx, 5xx)\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        # Only match <article> tag to get the main content\n",
        "        article = soup.find('article')\n",
        "        if not article:\n",
        "            print(f\"No <article> tag found for URL: {url}\")\n",
        "            return None\n",
        "\n",
        "        # Find all relevant text elements (<p> for paragraphs, <h1>-<h6> for headings)\n",
        "        tags = article.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
        "\n",
        "        # Extract text and join it into a single string\n",
        "        text = ''.join(tag.get_text(strip=True).rstrip('.\\n') + \".\\n\" for tag in tags)\n",
        "\n",
        "        return text\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error processing URL {url}: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "ePqVsg8kdtkp"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using only spaCy for tokenization, named entity recognition (NER), and part-of-speech (POS) tagging."
      ],
      "metadata": {
        "id": "iBwJu_qCemQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_with_spacy(text):\n",
        "    \"\"\"\n",
        "    Processes the given text using spaCy for tokenization, part-of-speech tagging,\n",
        "    and named entity recognition (NER).\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to process.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two elements:\n",
        "            - A list of proper noun tags (tokens with tag 'NNP').\n",
        "            - A list of filtered named entities (PERSON, ORG, GPE).\n",
        "    \"\"\"\n",
        "    doc = nlp(text)\n",
        "\n",
        "    sentences = list(doc.sents)\n",
        "\n",
        "    entities = []\n",
        "    nnp_tags = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "\n",
        "      for token in sentence:\n",
        "        if token.tag_ == 'NNP' and token.text.isalpha():\n",
        "          nnp_tags.append(token.text)\n",
        "\n",
        "      sentence_entities = []\n",
        "\n",
        "      sent_doc = nlp(sentence.text)\n",
        "      for ent in sent_doc.ents:\n",
        "        if ent.label_ in ['PERSON', 'ORG', 'GPE']:\n",
        "          entity = ent.text.strip()\n",
        "\n",
        "          if \"'s\" in entity:\n",
        "\n",
        "              cutoff = entity.index(\"'s\")\n",
        "\n",
        "              entity = entity[:cutoff]\n",
        "\n",
        "          if entity != '':\n",
        "\n",
        "              sentence_entities.append(entity)\n",
        "\n",
        "        sentence_entities = list(set(sentence_entities))\n",
        "\n",
        "        if len(sentence_entities) > 1:\n",
        "\n",
        "            entities.append(sentence_entities)\n",
        "\n",
        "    return nnp_tags, entities"
      ],
      "metadata": {
        "id": "9xTQ8nDXyHdg"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using spaCy and NLTK for tokenization, named entity recognition (NER), and part-of-speech (POS) tagging."
      ],
      "metadata": {
        "id": "U6Nf9jMVev0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_text_with_nltk_and_spacy(text):\n",
        "    \"\"\"\n",
        "    Process the given text using both NLTK and spaCy for tokenization,\n",
        "    part-of-speech tagging, and named entity recognition (NER).\n",
        "\n",
        "    Parameters:\n",
        "        text (str): The text to be processed.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing:\n",
        "            - A list of proper nouns (NNP) from POS tagging using NLTK.\n",
        "            - A list of filtered named entities (PERSON, ORG, GPE) using spaCy.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        #NLTK\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "\n",
        "        pos_tags = []\n",
        "        for sentence in sentences:\n",
        "            words = nltk.word_tokenize(sentence)\n",
        "            pos_tags.extend(nltk.pos_tag(words))\n",
        "\n",
        "        nnp_tags = [tag for tag in pos_tags if tag[1] == 'NNP' and tag[0].isalpha()]\n",
        "\n",
        "        #spaCy\n",
        "        doc = nlp(text)\n",
        "\n",
        "        sentences = list(doc.sents)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        for sentence in sentences:\n",
        "          sentence_entities = []\n",
        "\n",
        "          sent_doc = nlp(sentence.text)\n",
        "          for ent in sent_doc.ents:\n",
        "            if ent.label_ in ['PERSON', 'ORG', 'GPE']:\n",
        "              entity = ent.text.strip()\n",
        "\n",
        "              if \"'s\" in entity:\n",
        "\n",
        "                  cutoff = entity.index(\"'s\")\n",
        "\n",
        "                  entity = entity[:cutoff]\n",
        "\n",
        "              if entity != '':\n",
        "\n",
        "                  sentence_entities.append(entity)\n",
        "\n",
        "            sentence_entities = list(set(sentence_entities))\n",
        "\n",
        "            if len(sentence_entities) > 1:\n",
        "\n",
        "                entities.append(sentence_entities)\n",
        "\n",
        "        return nnp_tags, entities\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing text: {e}\")\n",
        "        return [], []\n"
      ],
      "metadata": {
        "id": "VX4CBKo970l-"
      },
      "execution_count": 234,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting entities into network data\n"
      ],
      "metadata": {
        "id": "HOBPog1wf-Fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract entities from articles_urls texts"
      ],
      "metadata": {
        "id": "E7l1YWypKa38"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_entities(articles_urls, print_all=False):\n",
        "    \"\"\"\n",
        "    Extracts named entities related to theme from a list of article URLs.\n",
        "\n",
        "    Parameters:\n",
        "        articles_urls (list of str): List of URLs to process.\n",
        "        print_all (bool): If True, prints URLs, entities and NNP Tags.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of named entities extracted from the articles.\n",
        "    \"\"\"\n",
        "    entities = []\n",
        "\n",
        "    for url in articles_url:\n",
        "        text = get_article_text(url)\n",
        "\n",
        "        if print_all:\n",
        "            print(f\"URL: {url}\")\n",
        "\n",
        "        if text:\n",
        "            #nnp_tags, article_entities = process_text_with_nltk_and_spacy(text)\n",
        "            nnp_tags, article_entities = process_text_with_spacy(text)\n",
        "            entities.extend(article_entities)\n",
        "\n",
        "            if print_all:\n",
        "                print(\"NNP Tags (Proper Nouns):\", nnp_tags)\n",
        "                print(f\"Filtered Named Entities (PERSON, ORG, GPE): {article_entities}\\n\")\n",
        "\n",
        "    return entities"
      ],
      "metadata": {
        "id": "5YR2QPamKZSo"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get network data from entities"
      ],
      "metadata": {
        "id": "rql3opC2MQPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_network_data(entities):\n",
        "\n",
        "    final_sources = []\n",
        "    final_targets = []\n",
        "\n",
        "    for row in entities:\n",
        "\n",
        "        source = row[0]\n",
        "        targets = row[1:]\n",
        "\n",
        "        for target in targets:\n",
        "\n",
        "            final_sources.append(source)\n",
        "            final_targets.append(target)\n",
        "\n",
        "    df = pd.DataFrame({'source':final_sources, 'target':final_targets})\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "R7GonV0hL-zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Draw networkx graphs from network data"
      ],
      "metadata": {
        "id": "3gld6PBaPQOp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_graph(G, show_names=False, node_size=1, font_size=10, edge_width=0.5):\n",
        "\n",
        "    adjacency = nx.adjacency_matrix(G, weight='weight')\n",
        "    adjacency = sp.csr_matrix(adjacency)\n",
        "\n",
        "    names = np.array(list(G.nodes()))\n",
        "\n",
        "    graph = Bunch()\n",
        "    graph.adjacency = adjacency\n",
        "    graph.names = np.array(names)\n",
        "\n",
        "    pagerank = PageRank()\n",
        "\n",
        "    pagerank.fit(adjacency)\n",
        "    scores = pagerank.scores_\n",
        "\n",
        "    if show_names:\n",
        "\n",
        "        image = svg_graph(graph.adjacency, font_size=font_size, node_size=node_size, names=graph.names, width=700, height=500, scores=scores, edge_width=edge_width)\n",
        "\n",
        "    else:\n",
        "\n",
        "        image = svg_graph(graph.adjacency, node_size=node_size, width=700, height=500, scores = scores, edge_width=edge_width)\n",
        "\n",
        "    return SVG(image)"
      ],
      "metadata": {
        "id": "Imr9fk0zPYDb"
      },
      "execution_count": 247,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_ego_graph(G, ego, center=True, k=0, show_names=True, edge_width=0.1, node_size=3, font_size=12):\n",
        "\n",
        "    G.remove_edges_from(nx.selfloop_edges(G))\n",
        "\n",
        "    ego = nx.ego_graph(G, ego, center=center)\n",
        "\n",
        "    ego = nx.k_core(ego, k)\n",
        "\n",
        "    return draw_graph(ego, node_size=node_size, font_size=font_size, show_names=show_names, edge_width=edge_width)"
      ],
      "metadata": {
        "id": "TSI9PLESRFZQ"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure environment, set query and apply functions to get graph and metrics"
      ],
      "metadata": {
        "id": "ctk7SlvYNhjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "api_key = os.getenv(\"API_KEY\", \"test\")\n",
        "\n",
        "query = \"bitcoin\"\n",
        "\n",
        "tag = search_tags(query, api_key)[0]  # Get first tag\n",
        "section = \"money\"  # You can also use technology, economy or omit it\n",
        "\n",
        "articles_url = get_latest_guardian_articles_urls(query, api_key, page_size=100, tag=tag, section=None)\n",
        "\n",
        "print(f\"Número de artigos utilizados: {len(articles_url)}\")\n",
        "\n",
        "bitcoin_entities = extract_entities(articles_url, print_all=False)\n",
        "\n",
        "bitcoin_network_df = get_network_data(bitcoin_entities)\n",
        "\n",
        "G_bitcoin = nx.from_pandas_edgelist(bitcoin_network_df)\n"
      ],
      "metadata": {
        "id": "m5yGS9eJO4xJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940d8d3c-a0eb-44ed-92bc-3a5abf96e602"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de artigos utilizados: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "draw_ego_graph(G_bitcoin, 'Bitcoin')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 560
        },
        "id": "VLEjIr0eRd6L",
        "outputId": "80cd7920-e67a-45c3-fa56-1c1a57629703"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" width=\"855.81518924558\" height=\"540\">\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 720 215 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 343 520 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 117 291 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 117 291 116 199\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 564 77 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 359 41 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 653 298 720 215\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 653 298 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 653 298 556 277\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 653 298 603 361\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 653 298 472 372\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 243 48 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 603 361 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 603 361 556 277\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 149 105 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 603 361 653 298\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 20 263 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 666 442 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 498 472 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 498 472 472 372\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 328 435 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 328 435 472 372\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 472 372 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 472 372 556 277\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 472 372 281 350\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 472 372 653 298\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 472 372 603 361\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 472 372 498 472\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 472 372 328 435\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 603 361 472 372\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 281 350 472 372\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 281 350 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 475 20 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 720 215 679 136\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 720 215 653 298\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 720 215\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 556 277\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 679 136\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 135 430\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 475 20\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 281 350\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 149 105\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 343 520\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 117 291\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 564 77\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 359 41\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 653 298\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 243 48\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 603 361\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 20 263\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 666 442\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 498 472\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 328 435\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 472 372\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 391 260 116 199\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 556 277 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 556 277 653 298\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 556 277 603 361\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 556 277 472 372\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 679 136 720 215\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 679 136 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 135 430 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 116 199 391 260\"/>\n<path stroke-width=\"0.1\" stroke=\"gray\" d=\"M 116 199 117 291\"/>\n<circle cx=\"720\" cy=\"215\" r=\"3.0\" style=\"fill:rgb(86, 115, 224);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"391\" cy=\"260\" r=\"3.0\" style=\"fill:rgb(179, 3, 38);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"556\" cy=\"277\" r=\"3.0\" style=\"fill:rgb(96, 128, 232);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"679\" cy=\"136\" r=\"3.0\" style=\"fill:rgb(70, 93, 207);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"135\" cy=\"430\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"475\" cy=\"20\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"281\" cy=\"350\" r=\"3.0\" style=\"fill:rgb(70, 93, 207);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"149\" cy=\"105\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"343\" cy=\"520\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"117\" cy=\"291\" r=\"3.0\" style=\"fill:rgb(72, 96, 209);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"564\" cy=\"77\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"359\" cy=\"41\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"653\" cy=\"298\" r=\"3.0\" style=\"fill:rgb(109, 144, 241);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"243\" cy=\"48\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"603\" cy=\"361\" r=\"3.0\" style=\"fill:rgb(96, 128, 232);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"20\" cy=\"263\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"666\" cy=\"442\" r=\"3.0\" style=\"fill:rgb(58, 76, 192);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"498\" cy=\"472\" r=\"3.0\" style=\"fill:rgb(70, 93, 207);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"328\" cy=\"435\" r=\"3.0\" style=\"fill:rgb(70, 93, 207);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"472\" cy=\"372\" r=\"3.0\" style=\"fill:rgb(145, 179, 254);stroke:black;stroke-width:1.0\"/>\n<circle cx=\"116\" cy=\"199\" r=\"3.0\" style=\"fill:rgb(72, 96, 209);stroke:black;stroke-width:1.0\"/>\n<text text-anchor=\"start\" x=\"726\" y=\"215\" font-size=\"12\">FBI</text><text text-anchor=\"start\" x=\"397\" y=\"260\" font-size=\"12\">Bitcoin</text><text text-anchor=\"start\" x=\"562\" y=\"277\" font-size=\"12\">America</text><text text-anchor=\"start\" x=\"685\" y=\"136\" font-size=\"12\">Gurvais Grigg</text><text text-anchor=\"start\" x=\"141\" y=\"430\" font-size=\"12\">Joseph James O’Connor</text><text text-anchor=\"start\" x=\"481\" y=\"20\" font-size=\"12\">Morgan</text><text text-anchor=\"start\" x=\"287\" y=\"350\" font-size=\"12\">UK</text><text text-anchor=\"start\" x=\"155\" y=\"105\" font-size=\"12\">Binance boss</text><text text-anchor=\"start\" x=\"349\" y=\"520\" font-size=\"12\">Brosens</text><text text-anchor=\"start\" x=\"123\" y=\"291\" font-size=\"12\">authorof</text><text text-anchor=\"start\" x=\"570\" y=\"77\" font-size=\"12\">David Gerard</text><text text-anchor=\"start\" x=\"365\" y=\"41\" font-size=\"12\">US Securities and Exchange Commission</text><text text-anchor=\"start\" x=\"659\" y=\"298\" font-size=\"12\">SEC</text><text text-anchor=\"start\" x=\"249\" y=\"48\" font-size=\"12\">Monero</text><text text-anchor=\"start\" x=\"609\" y=\"361\" font-size=\"12\">Trump</text><text text-anchor=\"start\" x=\"26\" y=\"263\" font-size=\"12\">Sam Bankman-Friedcommitted</text><text text-anchor=\"start\" x=\"672\" y=\"442\" font-size=\"12\">reserve”of</text><text text-anchor=\"start\" x=\"504\" y=\"472\" font-size=\"12\">Celsius</text><text text-anchor=\"start\" x=\"334\" y=\"435\" font-size=\"12\">Donald Trump</text><text text-anchor=\"start\" x=\"478\" y=\"372\" font-size=\"12\">US</text><text text-anchor=\"start\" x=\"122\" y=\"199\" font-size=\"12\">Satoshi Nakamoto</text></svg>"
          },
          "metadata": {},
          "execution_count": 270
        }
      ]
    }
  ]
}